{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The worker-ng framework has been developed to meet specific use cases: many small computations determined by parameter variations; the scheduler's task is easier when it does not have to deal with too many jobs. Such use cases often have a common root: the user wants to run a program with a large number of parameter settings, and the program does not allow for aggregation, i.e., it has to be run once for each instance of the parameter values. This how-to shows you how to use the worker-ng framework.","title":"Introduction and motivation"},{"location":"changes/","text":"Changes Nothing significant yet.","title":"Changes"},{"location":"changes/#changes","text":"Nothing significant yet.","title":"Changes"},{"location":"commands/","text":"worker-ng commands The worker-ng framework has the following commands: wsub : submit a worker job wresume : resubmit a worker job, however, only unfinished work items will be executed wsummarize : provide summary information on a (running) worker job, showing the number of completed or failed work items","title":"worker commands"},{"location":"commands/#worker-ng-commands","text":"The worker-ng framework has the following commands: wsub : submit a worker job wresume : resubmit a worker job, however, only unfinished work items will be executed wsummarize : provide summary information on a (running) worker job, showing the number of completed or failed work items","title":"worker-ng commands"},{"location":"contact/","text":"Contact & support Bug reports and feature request can be sent to the developer, Geert Jan Bex , Hasselt University, or preferably, be submitted as issues on GitHub . Although the code is publicly available on GitHub, it is nevertheless an internal tool, so no support under any form is guaranteed, although it may be provided. It is released under the terms and conditions of the GNU General public license, version 3.","title":"Contact information"},{"location":"contact/#contact-support","text":"Bug reports and feature request can be sent to the developer, Geert Jan Bex , Hasselt University, or preferably, be submitted as issues on GitHub . Although the code is publicly available on GitHub, it is nevertheless an internal tool, so no support under any form is guaranteed, although it may be provided. It is released under the terms and conditions of the GNU General public license, version 3.","title":"Contact &amp; support"},{"location":"efficiency/","text":"Worker is implemented using MPI, so it is not restricted to a single compute node, it scales well to many nodes. However, remember that jobs requesting a large number of nodes typically spend quite some time in the queue. Worker will be effective when * work items, i.e., individual computations, are neither too short, nor too long (i.e., from a few minutes to a few hours); and, * when the number of work items is larger than the number of CPUs involved in the job (e.g., more than 30 for 8 CPUs). Also note that the total execution time of a job consisting of work items that could be executed using multiple threads will be lower when using a single thread, provided the number of work items is larger than the number of cores. When using a prologue and/or an epilogue, bare in mind that those processes are executed by the master only, while all worker processes are in fact idle. This implies that prologue and epilogues only make sense when they required very little time compared to the actual parallel work to be performed. If execution times of prologue and/or epilogue are considerable, consider submitten jobs with dependencies instead.","title":"Efficiency"},{"location":"further_info/","text":"Further information This how-to introduces only Worker's basic features. All worker command has usage information that is printed when the --help option is specified, e.g., $ wsub --help","title":"Further information"},{"location":"further_info/#further-information","text":"This how-to introduces only Worker's basic features. All worker command has usage information that is printed when the --help option is specified, e.g., $ wsub --help","title":"Further information"},{"location":"mapreduce/","text":"Map-reduce scenarios Often, an embarrassingly parallel computation can be abstracted to three simple steps: a preparation phase in which the data is split up into smaller, more manageable chuncks; on these chuncks, the same algorithm is applied independently (these are the work items); and the results of the computations on those chuncks are aggregated into, e.g., a statistical description of some sort. The Worker-ng framework does not directly supports this scenario since it is easily implemented using job dependencies which are support by most schedulers, including Slurm.","title":"Prologue and epilogue"},{"location":"mapreduce/#map-reduce-scenarios","text":"Often, an embarrassingly parallel computation can be abstracted to three simple steps: a preparation phase in which the data is split up into smaller, more manageable chuncks; on these chuncks, the same algorithm is applied independently (these are the work items); and the results of the computations on those chuncks are aggregated into, e.g., a statistical description of some sort. The Worker-ng framework does not directly supports this scenario since it is easily implemented using job dependencies which are support by most schedulers, including Slurm.","title":"Map-reduce scenarios"},{"location":"monitoring/","text":"Monitoring worker-ng jobs You will have noticed that when you have submitted a job using wsub , a directory is created with a name that starts with worker_ , and ends in the job ID. Among other things, this directory contains files that allow you to monitor the progress of a running worker job, or analyze its performance once it is done. Since a worker-ng job will typically run for several hours, it may be reassuring to monitor its progress. worker server keeps a log of its activity in the directory mentioned abovewhere the job was submitted. You can use the wsummarize command to get information, e.g., for job ID was 1234 . $ wsummarize --dir=worker_1234/ This will give you an overview of the status of your work items, i.e., the number of succesful items: the number of computations that finished with exit status 0; failed items: the computations that finished with a non-zero exit status; incomplete items: the number of items that are currnetly being executed. To monitor progress \"in real time\", you can use the watch Linux command. $ watch -n 60 wsummarize --dir=worker_1234/ This will summarize the status of the work items every 60 seconds. Note: use a reasonable value for the update period, this will cause load on the login node where you run this command. The wsummarize command has various command line options to get a more detailed analysis of perfornmance issues. For instance, to get statistics on the walltime of your work items, you can use the --show_walltime_stats flag. This will give you descriptive statistics on the walltime of your work items such as the minimum and maximum, the average and median, as well as informaiton on the spread. In order to detect problems with load balancing between the worker clients, you can use the --show_client_stats flag. This will provide you with the same descriptive statistics on the walltime, but grouped by client. In addition, you will get the total walltime for each client, a good measure for load balance. Finally, the --show_all options will given the output of --show_walltime_stats and --show_client_stats in a single wsammarize invocation.","title":"Monitoring worker jobs"},{"location":"monitoring/#monitoring-worker-ng-jobs","text":"You will have noticed that when you have submitted a job using wsub , a directory is created with a name that starts with worker_ , and ends in the job ID. Among other things, this directory contains files that allow you to monitor the progress of a running worker job, or analyze its performance once it is done. Since a worker-ng job will typically run for several hours, it may be reassuring to monitor its progress. worker server keeps a log of its activity in the directory mentioned abovewhere the job was submitted. You can use the wsummarize command to get information, e.g., for job ID was 1234 . $ wsummarize --dir=worker_1234/ This will give you an overview of the status of your work items, i.e., the number of succesful items: the number of computations that finished with exit status 0; failed items: the computations that finished with a non-zero exit status; incomplete items: the number of items that are currnetly being executed. To monitor progress \"in real time\", you can use the watch Linux command. $ watch -n 60 wsummarize --dir=worker_1234/ This will summarize the status of the work items every 60 seconds. Note: use a reasonable value for the update period, this will cause load on the login node where you run this command. The wsummarize command has various command line options to get a more detailed analysis of perfornmance issues. For instance, to get statistics on the walltime of your work items, you can use the --show_walltime_stats flag. This will give you descriptive statistics on the walltime of your work items such as the minimum and maximum, the average and median, as well as informaiton on the spread. In order to detect problems with load balancing between the worker clients, you can use the --show_client_stats flag. This will provide you with the same descriptive statistics on the walltime, but grouped by client. In addition, you will get the total walltime for each client, a good measure for load balance. Finally, the --show_all options will given the output of --show_walltime_stats and --show_client_stats in a single wsammarize invocation.","title":"Monitoring worker-ng jobs"},{"location":"multithreading/","text":"Multithreaded work items If a work item uses threading, the number of threads for each work item can simply be specified by using the --cpus-per-task option, similar to a regular Slurm job. If you need to know the number of threads in your Slurm job script, you can use the SLURM_CPUS_PER_TASK_HET_GROUP_0 environment variable. This variable has to be used, rather than SLURM_CPUS_PER_TASK , because under the hood, worker-ng uses a heterogeneous job.","title":"Multithreaded work items"},{"location":"multithreading/#multithreaded-work-items","text":"If a work item uses threading, the number of threads for each work item can simply be specified by using the --cpus-per-task option, similar to a regular Slurm job. If you need to know the number of threads in your Slurm job script, you can use the SLURM_CPUS_PER_TASK_HET_GROUP_0 environment variable. This variable has to be used, rather than SLURM_CPUS_PER_TASK , because under the hood, worker-ng uses a heterogeneous job.","title":"Multithreaded work items"},{"location":"resume/","text":"Resuming worker-ng jobs Unfortunately, it is not always easy to estimate the walltime for a job, and consequently, sometimes the latter is underestimated. When using the worker framework, this implies that not all work items will have been processed. worker makes it very easy to resume such a job without having to figure out which work items did complete successfully, and which remain to be computed. Suppose the job that did not complete all its work items had ID '1234'. $ wresume --dir=worker_1234 This will submit a new job that will start to work on the work items that were not done yet. Note that it is possible to change almost all job parameters when resuming, specifically the requested resources such as the number of cores and the walltime. $ wresume --time=1:30:00 --dir=worker_1234 Work items may fail to complete successfully for a variety of reasons, e.g., a data file that is missing, a (minor) programming error, etc. Upon resuming a job, the work items that failed are considered to be done, so resuming a job will only execute work items that did not terminate when the job ended, either because they were being executed at that point, or had notstarted yet. It is very easy to get a list of the work items that fail using wsummarize . $ wsummarize --dir=worker_1234 --show_failed Using the work item IDs you can inspect the data to try to find the cause of the failure. Once you have identified and solved the problem, It is you can redo work items that failed easily. $ wresume --dir=worker_1234 --redo","title":"Resuming a worker job"},{"location":"resume/#resuming-worker-ng-jobs","text":"Unfortunately, it is not always easy to estimate the walltime for a job, and consequently, sometimes the latter is underestimated. When using the worker framework, this implies that not all work items will have been processed. worker makes it very easy to resume such a job without having to figure out which work items did complete successfully, and which remain to be computed. Suppose the job that did not complete all its work items had ID '1234'. $ wresume --dir=worker_1234 This will submit a new job that will start to work on the work items that were not done yet. Note that it is possible to change almost all job parameters when resuming, specifically the requested resources such as the number of cores and the walltime. $ wresume --time=1:30:00 --dir=worker_1234 Work items may fail to complete successfully for a variety of reasons, e.g., a data file that is missing, a (minor) programming error, etc. Upon resuming a job, the work items that failed are considered to be done, so resuming a job will only execute work items that did not terminate when the job ended, either because they were being executed at that point, or had notstarted yet. It is very easy to get a list of the work items that fail using wsummarize . $ wsummarize --dir=worker_1234 --show_failed Using the work item IDs you can inspect the data to try to find the cause of the failure. Once you have identified and solved the problem, It is you can redo work items that failed easily. $ wresume --dir=worker_1234 --redo","title":"Resuming worker-ng jobs"},{"location":"steps/","text":"worker-ng step-by-step As prerequisites, one should have a (sequential) job that has to be run many times for various parameter values, i.e., parameter exploration; or on a large number of input files. Parameter exploration By way of running example, we will use a Python script sum.py that simply computes the sum of two numbers given on the command line: #!/usr/bin/env python import argparse import sys def main(): arg_parser = argparse.ArgumentParser(description='sum two values') arg_parser.add_argument('-a', type=float, required=True, help='A value') arg_parser.add_argument('-b', type=float, required=True, help='B value') options = arg_parser.parse_args() print(options.a + options.b) return 0 if __name__ == '__main__': sys.exit(main()) On the command line, we would run this as follows: python sum.py -a=1.3 -b=2.5 The program will write its results to standard output. A slurm script (say sum.slurm ) that would run this as a job would then look like: #!/usr/bin/env -S bash -l #SBATCH --account=my_account #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:02:00 python sum.py -a 1.3 -b 2.5 When we submit this job, the calculation is performed for this particular instance of the parameters, i.e., a = 1.3 and b = 2.5 . To submit the job, the user would use: $ sbatch sum.slurm However, the user wants to run this program for many parameter instances, e.g., he wants to run the program on 100 instances of a and b . To this end, the Slurm file can be modified as follows. #!/usr/bin/env -S bash -l #SBATCH --account=my_account #SBATCH --nodes=1 #SBATCH --ntasks=10 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 python sum.py -a=$a -b=$b Note that * the parameter values 1.3 and 2.5 are replaced by variables $a and $b respectively; * the number of tasks, i.e., cores per node has been increased to 10, i.e., --ntasks=1 is replaced by --ntasks=10 ; and * the walltime has been increased to 20 mintues, i.e., --time=00:02:00 is replaced by --time=00:20:00 . The walltime is calculated as follows: one calculation takes 2 minutes, so 100 calculations take 200 minutes on one core. However, this job will use 10 cores, so the 100 calculations will be done in 200/10 = 20 minutes. Note that one core will always be required for management purposes, so if we specify --ntasks=10 and --cpus-per-task=1 , the job will required 11 cores from the scheduler. This is done automatically by the worker-ng framework though. The 100 parameter instances can be stored in a Comma Separated Value file (CSV) that can be generated using a spreadsheet program such as Microsoft Excel, or just by hand using any text editor (do not use a word processor such as Microsoft Word though). The first few lines of the file data.csv would look as follows. a,b 1.3,2.5 1.05,4.3 1.05,4.3 1.15,4.3 1.25,4.3 ... It has to contain the names of the variables on the first line, i.e., a and b in this example, followed by 100 parameter instances in the current example, each on a line by itself. Values on a line are separated by commas. The job can now be submitted as follows. $ module load worker-ng $ wsub --batch=sum.slurm --data=data.txt Note that the Slurm file is the value of the --batch option . The sum.py scripts program will now be run for all 100 parameter instances--\u201410 concurrently--\u2014until all computations are done. A computation for such a parameter instance is called a work item in worker-ng parlance. Job arrays Most schedulers including Slurm support job arrays. However, often they are configured so that users can only have a fairly small number of jobs in the queue. This implies that the size of job arrays is subject to that same limit. Although this makes sense from the point of view of system administrators who don't want to have their schedulers overloaded, it is sometimes less than convenient for users who want to have job arrays with many jobs. worker offers a way around this by \"packing\" the job array into a single job, hence allowing for large job arrays that do not impact the scheduler's performance in any way. As an example, we will consider the following use case. We have a directory data that contains 100 text files, named text-001.txt to text-100.txt . We want to count the words in each of these files, and want to do the work in parallel. On the command line, we would to this as follows for a single file. $ echo -n \"text-001.txt: \" && cat data/text-001.txt | wc -w A typical slurm job script wc.slurm for use with job arrays would look as follows. #!/usr/bin/env -S bash -l #SBATCH --account=lpt2_sysadmin #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:02:00 # create an output directory if it doesn't exist mkdir -p output # define input and output file names based on the array ID INPUT_FILE=\"data/file_$(printf '%04d' $SLURM_ARRAY_TASK_ID).txt\" OUTPUT_FILE=\"output/count_$(printf '%04d' $SLURM_ARRAY_TASK_ID).txt\" # count the words and write the result in the output file echo -n \"$INPUT_FILE: \" > $OUTPUT_FILE cat $INPUT_FILE | wc -w >> $OUTPUT_FILE We can submit this as a standard slurm job array as follows. $ sbatch --array=1-100 wc.slurm However, this would result in 100 arrays jobs, which may exceed the limitations set by the configuration of the scheduler. Using worker, a feature akin to job arrays can be used with minimal modifications to the job script. #!/usr/bin/env -S bash -l #SBATCH --account=lpt2_sysadmin #SBATCH --nodes=1 #SBATCH --ntasks=10 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # create an output directory if it doesn't exist mkdir -p output # define input and output file names based on the array ID INPUT_FILE=\"data/file_$(printf '%04d' $SLURM_ARRAY_TASK_ID).txt\" OUTPUT_FILE=\"output/count_$(printf '%04d' $SLURM_ARRAY_TASK_ID).txt\" # count the words and write the result in the output file echo -n \"$INPUT_FILE: \" > $OUTPUT_FILE cat $INPUT_FILE | wc -w >> $OUTPUT_FILE Note that * the number of tasks, i.e., cores per node has been increased to 10, i.e., --ntasks=1 is replaced by --ntasks=10 ; and * the walltime has been increased to 20 mintues, i.e., --time=00:02:00 is replaced by --time=00:20:00 . The walltime is calculated as follows: one calculation takes 2 minutes, so 100 calculations take 200 minutes on one core. However, this job will use 10 cores, so the 100 calculations will be done in 200/10 = 20 minutes. The job is now submitted as follows. $ module load worker-ng $ wsub --array=1-100 --batch=wc.slurm The wc program will now be run for all 100 input files--\u201410 concurrently--\u2014until all computations are done. Again, a computation for an individual input file, or, equivalently, an array ID, is called a work item in worker-ng speak. Note that in constrast to Slurm job arrays, a worker job array submits a single job only.","title":"Step by step"},{"location":"steps/#worker-ng-step-by-step","text":"As prerequisites, one should have a (sequential) job that has to be run many times for various parameter values, i.e., parameter exploration; or on a large number of input files.","title":"worker-ng step-by-step"},{"location":"steps/#parameter-exploration","text":"By way of running example, we will use a Python script sum.py that simply computes the sum of two numbers given on the command line: #!/usr/bin/env python import argparse import sys def main(): arg_parser = argparse.ArgumentParser(description='sum two values') arg_parser.add_argument('-a', type=float, required=True, help='A value') arg_parser.add_argument('-b', type=float, required=True, help='B value') options = arg_parser.parse_args() print(options.a + options.b) return 0 if __name__ == '__main__': sys.exit(main()) On the command line, we would run this as follows: python sum.py -a=1.3 -b=2.5 The program will write its results to standard output. A slurm script (say sum.slurm ) that would run this as a job would then look like: #!/usr/bin/env -S bash -l #SBATCH --account=my_account #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:02:00 python sum.py -a 1.3 -b 2.5 When we submit this job, the calculation is performed for this particular instance of the parameters, i.e., a = 1.3 and b = 2.5 . To submit the job, the user would use: $ sbatch sum.slurm However, the user wants to run this program for many parameter instances, e.g., he wants to run the program on 100 instances of a and b . To this end, the Slurm file can be modified as follows. #!/usr/bin/env -S bash -l #SBATCH --account=my_account #SBATCH --nodes=1 #SBATCH --ntasks=10 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 python sum.py -a=$a -b=$b Note that * the parameter values 1.3 and 2.5 are replaced by variables $a and $b respectively; * the number of tasks, i.e., cores per node has been increased to 10, i.e., --ntasks=1 is replaced by --ntasks=10 ; and * the walltime has been increased to 20 mintues, i.e., --time=00:02:00 is replaced by --time=00:20:00 . The walltime is calculated as follows: one calculation takes 2 minutes, so 100 calculations take 200 minutes on one core. However, this job will use 10 cores, so the 100 calculations will be done in 200/10 = 20 minutes. Note that one core will always be required for management purposes, so if we specify --ntasks=10 and --cpus-per-task=1 , the job will required 11 cores from the scheduler. This is done automatically by the worker-ng framework though. The 100 parameter instances can be stored in a Comma Separated Value file (CSV) that can be generated using a spreadsheet program such as Microsoft Excel, or just by hand using any text editor (do not use a word processor such as Microsoft Word though). The first few lines of the file data.csv would look as follows. a,b 1.3,2.5 1.05,4.3 1.05,4.3 1.15,4.3 1.25,4.3 ... It has to contain the names of the variables on the first line, i.e., a and b in this example, followed by 100 parameter instances in the current example, each on a line by itself. Values on a line are separated by commas. The job can now be submitted as follows. $ module load worker-ng $ wsub --batch=sum.slurm --data=data.txt Note that the Slurm file is the value of the --batch option . The sum.py scripts program will now be run for all 100 parameter instances--\u201410 concurrently--\u2014until all computations are done. A computation for such a parameter instance is called a work item in worker-ng parlance.","title":"Parameter exploration"},{"location":"steps/#job-arrays","text":"Most schedulers including Slurm support job arrays. However, often they are configured so that users can only have a fairly small number of jobs in the queue. This implies that the size of job arrays is subject to that same limit. Although this makes sense from the point of view of system administrators who don't want to have their schedulers overloaded, it is sometimes less than convenient for users who want to have job arrays with many jobs. worker offers a way around this by \"packing\" the job array into a single job, hence allowing for large job arrays that do not impact the scheduler's performance in any way. As an example, we will consider the following use case. We have a directory data that contains 100 text files, named text-001.txt to text-100.txt . We want to count the words in each of these files, and want to do the work in parallel. On the command line, we would to this as follows for a single file. $ echo -n \"text-001.txt: \" && cat data/text-001.txt | wc -w A typical slurm job script wc.slurm for use with job arrays would look as follows. #!/usr/bin/env -S bash -l #SBATCH --account=lpt2_sysadmin #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:02:00 # create an output directory if it doesn't exist mkdir -p output # define input and output file names based on the array ID INPUT_FILE=\"data/file_$(printf '%04d' $SLURM_ARRAY_TASK_ID).txt\" OUTPUT_FILE=\"output/count_$(printf '%04d' $SLURM_ARRAY_TASK_ID).txt\" # count the words and write the result in the output file echo -n \"$INPUT_FILE: \" > $OUTPUT_FILE cat $INPUT_FILE | wc -w >> $OUTPUT_FILE We can submit this as a standard slurm job array as follows. $ sbatch --array=1-100 wc.slurm However, this would result in 100 arrays jobs, which may exceed the limitations set by the configuration of the scheduler. Using worker, a feature akin to job arrays can be used with minimal modifications to the job script. #!/usr/bin/env -S bash -l #SBATCH --account=lpt2_sysadmin #SBATCH --nodes=1 #SBATCH --ntasks=10 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # create an output directory if it doesn't exist mkdir -p output # define input and output file names based on the array ID INPUT_FILE=\"data/file_$(printf '%04d' $SLURM_ARRAY_TASK_ID).txt\" OUTPUT_FILE=\"output/count_$(printf '%04d' $SLURM_ARRAY_TASK_ID).txt\" # count the words and write the result in the output file echo -n \"$INPUT_FILE: \" > $OUTPUT_FILE cat $INPUT_FILE | wc -w >> $OUTPUT_FILE Note that * the number of tasks, i.e., cores per node has been increased to 10, i.e., --ntasks=1 is replaced by --ntasks=10 ; and * the walltime has been increased to 20 mintues, i.e., --time=00:02:00 is replaced by --time=00:20:00 . The walltime is calculated as follows: one calculation takes 2 minutes, so 100 calculations take 200 minutes on one core. However, this job will use 10 cores, so the 100 calculations will be done in 200/10 = 20 minutes. The job is now submitted as follows. $ module load worker-ng $ wsub --array=1-100 --batch=wc.slurm The wc program will now be run for all 100 input files--\u201410 concurrently--\u2014until all computations are done. Again, a computation for an individual input file, or, equivalently, an array ID, is called a work item in worker-ng speak. Note that in constrast to Slurm job arrays, a worker job array submits a single job only.","title":"Job arrays"},{"location":"time_limits/","text":"Time limits Sometimes, the execution of a work item takes long than expected, or worse, some work items get stuck in an infinite loop. This situation is unfortunate, since it implies that work items that could successfully are not even started. Again, a simple and yet versatile solution is offered by the Worker framework. If we want to limit the execution of each work item to at most 20 minutes, this can be accomplished by modifying the script of the running example. #!/usr/bin/env -S bash -l #SBATCH --account=my_account #SBATCH --ntasks=8 #SBATCH time=04:00:00 module load timedrun timedrun -t 00:20:00 cfd_test -t $temperature -p $pressure -v $volume Note that it is trivial to set individual time constraints for work items by introducing a parameter, and including the values of the latter in the CSV file, along with those for the temperature, pressure and volume. Also note that 'timedrun' is in fact offered in a module of its own, so it can be used outside the worker-ng framework as well.","title":"Limiting execution time"},{"location":"time_limits/#time-limits","text":"Sometimes, the execution of a work item takes long than expected, or worse, some work items get stuck in an infinite loop. This situation is unfortunate, since it implies that work items that could successfully are not even started. Again, a simple and yet versatile solution is offered by the Worker framework. If we want to limit the execution of each work item to at most 20 minutes, this can be accomplished by modifying the script of the running example. #!/usr/bin/env -S bash -l #SBATCH --account=my_account #SBATCH --ntasks=8 #SBATCH time=04:00:00 module load timedrun timedrun -t 00:20:00 cfd_test -t $temperature -p $pressure -v $volume Note that it is trivial to set individual time constraints for work items by introducing a parameter, and including the values of the latter in the CSV file, along with those for the temperature, pressure and volume. Also note that 'timedrun' is in fact offered in a module of its own, so it can be used outside the worker-ng framework as well.","title":"Time limits"},{"location":"todo/","text":"To do This is the initial release, so some work is still to be done. Field testing; Add documentation and examples for multithreaded and MPI work items; Port areduce (part of atools) to worker and add documentation. In the long run a number of features are possible. Client-only jobs: clients can join a running server; Remote server setup: a worker server can run on any system; Dynamic workload: a server uses a database rather than a file to get its workloads, allowing for dynamic workloads.","title":"To do"},{"location":"todo/#to-do","text":"This is the initial release, so some work is still to be done. Field testing; Add documentation and examples for multithreaded and MPI work items; Port areduce (part of atools) to worker and add documentation. In the long run a number of features are possible. Client-only jobs: clients can join a running server; Remote server setup: a worker server can run on any system; Dynamic workload: a server uses a database rather than a file to get its workloads, allowing for dynamic workloads.","title":"To do"},{"location":"trouble/","text":"Troubleshooting The most common problem with the worker-ng framework is that it doesn't seem to work at all, showing messages in the error file about module failing to work. The cause is trivial, and easy to remedy. Like any Slurm script, a worker Slurm file has to be in UNIX format! If you edited a Slurm script on your desktop, or something went wrong during sftp/scp, the PBS file may end up in DOS/Windows format, i.e., it has the wrong line endings. The PBS/torque queue system can not deal with that, so you will have to convert the file, e.g., for file run.slurm : $ dos2unix run.slurm","title":"Troubleshooting"},{"location":"trouble/#troubleshooting","text":"The most common problem with the worker-ng framework is that it doesn't seem to work at all, showing messages in the error file about module failing to work. The cause is trivial, and easy to remedy. Like any Slurm script, a worker Slurm file has to be in UNIX format! If you edited a Slurm script on your desktop, or something went wrong during sftp/scp, the PBS file may end up in DOS/Windows format, i.e., it has the wrong line endings. The PBS/torque queue system can not deal with that, so you will have to convert the file, e.g., for file run.slurm : $ dos2unix run.slurm","title":"Troubleshooting"},{"location":"usage/","text":"Usage scenarios Job arrays The user prepares a PBS script that calls a program, e.g., cfd_solver , that takes an parameter file specified on the command line. The PBS file cfd.pbs could look something like: #!/bin/bash -l #PBS -N cfd_solver #PBS -l nodes=1 #PBS -l walltime=00:05:00 cd $PBS_O_WORKDIR cfd_solver -p parameters-$PBS_ARRAYID.cfg > result-$PBS_ARRAYID.dat For 100 parameter instances called parameters-1.cfg ,..., parameters-100.cfg , the following obsolete qsub command would run cfd_solver on each of those 100 parameter files, saving the results of each run in result-1.dat , ..., result-100.dat respectively: qsub -t 1-100 cfd.pbs Each job in the job array was assigned a unique number between 1 and 100 that is accessible via the shell variable $PBS_ARRAYID . Given that Moab does not support job arrays, Worker can now handle this setup transparently. $ module load worker $ wsub -t 1-100 -batch cfd.pbs -l nodes=8 Notice that you have to request the number of nodes you want worker to use, multiples of 8 will be most efficient. In this case, 7 nodes will do the work, so the speedup due to parallellization will be approximately 7. The wsub command takes all options that were valid for torque's qsub , notably the features and resources requested via the -l option. Parameter variations A fairly common usage scenario is similar to the previous one, except that the parameter instances are to be provided on the command line, rather than in a configuration file. By way of example, consider the parameters in the comma separated value (CSV) format in the file data.csv below: temperature,pressure,volume 293.0,1.0e6,1.0 294.0,1.0e6,1.0 295.0,1.0e6,1.0 296.0,1.0e6,1.0 ... These values can be supplied a program via the command line, e.g., simulate -t 293.0 -p 1.0e6 -v 1.0 . Now the user want to run simulate for each parameter set in data.csv< . First, the following PBS script simulate.pbs should be created: #!/bin/bash -l # PBS -N simulate simulate -t $temperature -p $pressure -v $volume Note that the variables $temperature , $pressure , and $volume correspond to the column names in data.csv The job can now be run using: $ module load worker $ wsub -data data.csv -batch simulate.pbs -l nodes=8 Notice that you have to request the number of nodes you want worker to use, multiples of 8 will be most efficient. In this case, 7 nodes will do the work, so the speedup due to parallellization will be approximately 7. The wsub command takes all options that were valid for torque's qsub , notably the features and resources requested via the -l option. MapReduce Using wsub 's -prolog and -epilog options, it is straightforward to implement MapReduce scenarios. The shell scripts passed through -prolog and -epilog are processed by the worker master before any work is started, and after all work has been completed, respectively. Hence the prolog shell script can be used to split the data in chuncks that can be handled by the slaves in parallel, while the epilog script can collect the results and postprocess them.","title":"Usage scenarios"},{"location":"usage/#usage-scenarios","text":"","title":"Usage scenarios"},{"location":"usage/#job-arrays","text":"The user prepares a PBS script that calls a program, e.g., cfd_solver , that takes an parameter file specified on the command line. The PBS file cfd.pbs could look something like: #!/bin/bash -l #PBS -N cfd_solver #PBS -l nodes=1 #PBS -l walltime=00:05:00 cd $PBS_O_WORKDIR cfd_solver -p parameters-$PBS_ARRAYID.cfg > result-$PBS_ARRAYID.dat For 100 parameter instances called parameters-1.cfg ,..., parameters-100.cfg , the following obsolete qsub command would run cfd_solver on each of those 100 parameter files, saving the results of each run in result-1.dat , ..., result-100.dat respectively: qsub -t 1-100 cfd.pbs Each job in the job array was assigned a unique number between 1 and 100 that is accessible via the shell variable $PBS_ARRAYID . Given that Moab does not support job arrays, Worker can now handle this setup transparently. $ module load worker $ wsub -t 1-100 -batch cfd.pbs -l nodes=8 Notice that you have to request the number of nodes you want worker to use, multiples of 8 will be most efficient. In this case, 7 nodes will do the work, so the speedup due to parallellization will be approximately 7. The wsub command takes all options that were valid for torque's qsub , notably the features and resources requested via the -l option.","title":"Job arrays"},{"location":"usage/#parameter-variations","text":"A fairly common usage scenario is similar to the previous one, except that the parameter instances are to be provided on the command line, rather than in a configuration file. By way of example, consider the parameters in the comma separated value (CSV) format in the file data.csv below: temperature,pressure,volume 293.0,1.0e6,1.0 294.0,1.0e6,1.0 295.0,1.0e6,1.0 296.0,1.0e6,1.0 ... These values can be supplied a program via the command line, e.g., simulate -t 293.0 -p 1.0e6 -v 1.0 . Now the user want to run simulate for each parameter set in data.csv< . First, the following PBS script simulate.pbs should be created: #!/bin/bash -l # PBS -N simulate simulate -t $temperature -p $pressure -v $volume Note that the variables $temperature , $pressure , and $volume correspond to the column names in data.csv The job can now be run using: $ module load worker $ wsub -data data.csv -batch simulate.pbs -l nodes=8 Notice that you have to request the number of nodes you want worker to use, multiples of 8 will be most efficient. In this case, 7 nodes will do the work, so the speedup due to parallellization will be approximately 7. The wsub command takes all options that were valid for torque's qsub , notably the features and resources requested via the -l option.","title":"Parameter variations"},{"location":"usage/#mapreduce","text":"Using wsub 's -prolog and -epilog options, it is straightforward to implement MapReduce scenarios. The shell scripts passed through -prolog and -epilog are processed by the worker master before any work is started, and after all work has been completed, respectively. Hence the prolog shell script can be used to split the data in chuncks that can be handled by the slaves in parallel, while the epilog script can collect the results and postprocess them.","title":"MapReduce"}]}